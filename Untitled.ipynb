{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Float, func, desc\n",
    "from sqlalchemy.orm import Session\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.ext.automap import automap_base\n",
    "from flask import Flask, jsonify\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from flask_cors import CORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "username = 'postgres'\n",
    "password = 'Ripp@198801*8688'\n",
    "database = 'troll_tweet_project'\n",
    "#connect to local SQL db\n",
    "engine = create_engine(f'postgresql://{username}:{password}@localhost/{database}')\n",
    "\n",
    "#create variable to connect to postgress db\n",
    "connection = engine.connect()\n",
    "\n",
    "#reflect an existing database into a new model\n",
    "Base = automap_base()\n",
    "# reflect the tables\n",
    "Base.prepare(engine, reflect=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save reference to the table\n",
    "Tweets = Base.classes.tweets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<flask_cors.extension.CORS at 0x1ab59eb7a08>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app = Flask(__name__)\n",
    "#allow Cross Origin Resource Sharing\n",
    "CORS(app)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "View function mapping is overwriting an existing endpoint function: tweet_length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-58f5a4575c31>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#this route will return the average tweet length for each tweet that was tweeted per day\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;33m@\u001b[0m\u001b[0mapp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/api/data/tweet_len/<start>/<end>'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtweet_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m#set pattern for date formats going into and out of epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtime_pattern\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'%Y-%m-%d'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3.0\\lib\\site-packages\\flask\\app.py\u001b[0m in \u001b[0;36mdecorator\u001b[1;34m(f)\u001b[0m\n\u001b[0;32m   1312\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1313\u001b[0m             \u001b[0mendpoint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"endpoint\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1314\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_url_rule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1315\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3.0\\lib\\site-packages\\flask\\app.py\u001b[0m in \u001b[0;36mwrapper_func\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     96\u001b[0m                 \u001b[1;34m\"before the application starts serving requests.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m             )\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mupdate_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapper_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3.0\\lib\\site-packages\\flask\\app.py\u001b[0m in \u001b[0;36madd_url_rule\u001b[1;34m(self, rule, endpoint, view_func, provide_automatic_options, **options)\u001b[0m\n\u001b[0;32m   1281\u001b[0m                 raise AssertionError(\n\u001b[0;32m   1282\u001b[0m                     \u001b[1;34m\"View function mapping is overwriting an \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1283\u001b[1;33m                     \u001b[1;34m\"existing endpoint function: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1284\u001b[0m                 )\n\u001b[0;32m   1285\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview_functions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mview_func\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: View function mapping is overwriting an existing endpoint function: tweet_length"
     ]
    }
   ],
   "source": [
    "\n",
    "#this route will return the average tweet length for each tweet that was tweeted per day\n",
    "@app.route('/api/data/tweet_len/<start>/<end>')\n",
    "def tweet_length(start, end):\n",
    "    #set pattern for date formats going into and out of epoch\n",
    "    time_pattern = '%Y-%m-%d'\n",
    "    #connect to postgresql db\n",
    "    session = Session(engine)\n",
    "    #convert start input to epoch\n",
    "    start_converted = int(time.mktime(time.strptime(start, time_pattern)))\n",
    "    end_converted = int(time.mktime(time.strptime(end, time_pattern)))\n",
    "    #select the average of the length of the tweets, and the date\n",
    "    sel = [func.avg(func.length(Tweets.content)), Tweets.published_date]\n",
    "    #do a query of the sel, filter by between the start and end dates, and group by the date\n",
    "    result = session.query(*sel).filter(Tweets.published_date >= start_converted).filter(Tweets.published_date <= end_converted).group_by(Tweets.published_date).order_by(Tweets.published_date)\n",
    "    #close connection to db\n",
    "    session.close()\n",
    "    #create empty list for results\n",
    "    results = []\n",
    "    #create empty list for dates\n",
    "    dates = []\n",
    "    #create empty list for averages\n",
    "    averages = []\n",
    "    for tweet_length, tweet_date in result:\n",
    "        #convert out to epoch format\n",
    "        tweet_date_converted = dt.datetime.fromtimestamp(tweet_date).strftime('%Y-%m-%d')\n",
    "        #add converted date to dates list\n",
    "        dates.append(tweet_date_converted)\n",
    "        #first convert to float\n",
    "        tweet_len = float(tweet_length)\n",
    "        #add tweet length to list for averages\n",
    "        averages.append(tweet_len)\n",
    "    #create empty dictionary\n",
    "    result_dictionary = {}\n",
    "    #add dates list to dictionary\n",
    "    result_dictionary['dates'] = dates\n",
    "    #add tweet average length list to dictionary\n",
    "    result_dictionary['tweet_length'] = averages\n",
    "    #add dictionary to results list\n",
    "    results.append(result_dictionary)\n",
    "    #display in readable format\n",
    "    return jsonify(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#this route will return the average number of followers each account has that tweeted each day\n",
    "@app.route('/api/data/followers/<start>/<end>')\n",
    "def num_followers(start, end):\n",
    "    #set pattern for date formats going into and out of epoch\n",
    "    time_pattern = '%Y-%m-%d'\n",
    "    #connect to postgresql db\n",
    "    session = Session(engine)\n",
    "    #convert start input to epoch\n",
    "    start_converted = int(time.mktime(time.strptime(start, time_pattern)))\n",
    "    end_converted = int(time.mktime(time.strptime(end, time_pattern)))\n",
    "    #write sql query to find the most followers each account had per day that tweeted, between the start and end dates\n",
    "    query_text = f'select max(followers) as followers, author, published_date from tweets where published_date < {end_converted} and published_date > {start_converted}  group by published_date, author order by published_date;'\n",
    "    #read in the results from the above query into a new df\n",
    "    df_followers = pd.read_sql(query_text, engine)\n",
    "    #push the above df into a new table, and drop this table if it already exists\n",
    "    df_followers.to_sql('avg_followers', con=engine, index=False, if_exists='replace')\n",
    "    #add a primary key to above table so sqlalchemy can perform queries on the table\n",
    "    engine.execute('ALTER TABLE avg_followers ADD COLUMN internal_number  SERIAL PRIMARY KEY;')\n",
    "    #get updated reflection on the tables\n",
    "    Base.prepare(engine, reflect=True)\n",
    "    \n",
    "    engine.execute('ALTER TABLE avg_followers ALTER COLUMN followers TYPE INT USING followers::integer')\n",
    "    \n",
    "    Base.prepare(engine, reflect=True)\n",
    "    \n",
    "    #assign variable for the Avg_followers table to run queries on it\n",
    "    Avg_followers = Base.classes.avg_followers\n",
    "    #selection for query\n",
    "    sel = [func.avg(Avg_followers.followers), Avg_followers.published_date]\n",
    "    #group by the published date\n",
    "    result = session.query(*sel).group_by(Avg_followers.published_date)    \n",
    "    #close connection to db\n",
    "    session.close()\n",
    "    #create empty list for results\n",
    "    results = []\n",
    "    #create empty list for dates\n",
    "    dates = []\n",
    "    #create empty list for number of followers\n",
    "    followers = []\n",
    "    for avg_followers, tweet_date in result:\n",
    "        #convert out to epoch format\n",
    "        tweet_date_converted = dt.datetime.fromtimestamp(tweet_date).strftime('%Y-%m-%d')\n",
    "        #add converted date to dates list\n",
    "        dates.append(tweet_date_converted)\n",
    "        #first convert to float\n",
    "        average_followers = float(avg_followers)\n",
    "        #add tweet length to list for averages\n",
    "        followers.append(average_followers)\n",
    "    #create empty dictionary\n",
    "    result_dictionary = {}\n",
    "    #add dates list to dictionary\n",
    "    result_dictionary['dates'] = dates\n",
    "    #add tweet average length list to dictionary\n",
    "    result_dictionary['average_num_of_followers'] = averages\n",
    "    #add dictionary to results list\n",
    "    results.append(result_dictionary)\n",
    "    #display in readable format\n",
    "    return jsonify(results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#this route will return the average number of followers each account has that tweeted each day\n",
    "@app.route('/api/data/following/<start>/<end>')\n",
    "def num_following(start, end):\n",
    "    #todo - change comments to reflect following vs followers\n",
    "    #set pattern for date formats going into and out of epoch\n",
    "    time_pattern = '%Y-%m-%d'\n",
    "    #connect to postgresql db\n",
    "    session = Session(engine)\n",
    "    #convert start input to epoch\n",
    "    start_converted = int(time.mktime(time.strptime(start, time_pattern)))\n",
    "    end_converted = int(time.mktime(time.strptime(end, time_pattern)))\n",
    "    #write sql query to find the most followers each account had per day that tweeted, between the start and end dates\n",
    "    query_text = f'select max(following) as follow, author, published_date from tweets where published_date < {end_converted} and published_date > {start_converted}  group by published_date, author order by published_date;'\n",
    "    #read in the results from the above query into a new df\n",
    "    df_followers = pd.read_sql(query_text, engine)\n",
    "    #push the above df into a new table, and drop this table if it already exists\n",
    "    df_followers.to_sql('avg_following', con=engine, index=False, if_exists='replace')\n",
    "    #add a primary key to above table so sqlalchemy can perform queries on the table\n",
    "    engine.execute('ALTER TABLE avg_following ADD COLUMN internal_number  SERIAL PRIMARY KEY;')\n",
    "    #get updated reflection on the tables\n",
    "    Base.prepare(engine, reflect=True)\n",
    "\n",
    "#     engine.execute('ALTER TABLE avg_following ALTER COLUMN follow FLOAT;')\n",
    "\n",
    "#     Base.prepare(engine, reflect=True)\n",
    "    #assign variable for the Avg_followers table to run queries on it\n",
    "    Avg_following = Base.classes.avg_following\n",
    "    #selection for query\n",
    "    sel = [func.avg(Avg_following.follow), Avg_following.published_date]\n",
    "    #group by the published date\n",
    "    result = session.query(*sel).group_by(Avg_following.published_date)    \n",
    "    #close connection to db\n",
    "    session.close()\n",
    "    #create empty list for results\n",
    "    results = []\n",
    "    #create empty list for dates\n",
    "    dates = []\n",
    "    #create empty list for number of followers\n",
    "    followingg = []\n",
    "\n",
    "    averages = []\n",
    "    for avg_followin, tweet_date in result:\n",
    "        #convert out to epoch format\n",
    "        tweet_date_converted = dt.datetime.fromtimestamp(tweet_date).strftime('%Y-%m-%d')\n",
    "        #add converted date to dates list\n",
    "        dates.append(tweet_date_converted)\n",
    "        #first convert to float\n",
    "        avg_followinn = float(avg_followin)\n",
    "        #add tweet length to list for averages\n",
    "        followingg.append(avg_followinn)\n",
    "    #create empty dictionary\n",
    "    result_dictionary = {}\n",
    "    #add dates list to dictionary\n",
    "    result_dictionary['dates'] = dates\n",
    "    #add tweet average length list to dictionary\n",
    "    result_dictionary['average_num_following'] = averages\n",
    "    #add dictionary to results list\n",
    "    results.append(result_dictionary)\n",
    "    #display in readable format\n",
    "    return jsonify(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#this endpoint will return a list of unique twitter handles in our dataset\n",
    "@app.route('/api/data/accounts')\n",
    "def account_list():\n",
    "    session = Session(engine)\n",
    "    sel = [Tweets.author]\n",
    "    result = session.query(*sel).distinct()\n",
    "    session.close()\n",
    "    accounts = []\n",
    "    results = []\n",
    "    for handle in result:\n",
    "        accounts.append(handle[0])\n",
    "    result_dictionary = {}\n",
    "    result_dictionary['twitter_handle'] = accounts\n",
    "    results.append(result_dictionary)\n",
    "    return jsonify(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#this endpoint will data about a specific queried account\n",
    "@app.route('/api/data/account_dashbord/<account>')\n",
    "def account_dashbord(account):\n",
    "    session = Session(engine)\n",
    "    sel = [Tweets.author, func.max(Tweets.published_date), func.min(Tweets.published_date), func.count(Tweets.internal_tweet_number), Tweets.account_category, func.avg(Tweets.updates)]\n",
    "    result = session.query(*sel).filter(Tweets.author == account).group_by(Tweets.author, Tweets.account_category).all()\n",
    "    session.close()\n",
    "    results = []\n",
    "    result_dictionary = []\n",
    "    for author, max_tweet_date, min_tweet_date, num_tweets, category, interactions in result:\n",
    "        result_dictionary['twitter_handle'] = author\n",
    "        max_tweet_date1 = dt.datetime.fromtimestamp(max_tweet_date).strftime('%Y-%m-%d')\n",
    "        result_dictionary['latest_tweet'] = max_tweet_date1\n",
    "        min_tweet_date1 = dt.datetime.fromtimestamp(min_tweet_date).strftime('%Y-%m-%d')\n",
    "        result_dictionary['earliest_tweet'] = min_tweet_date1\n",
    "        result_dictionary['number_of_tweets'] = float(num_tweets)\n",
    "        result_dictionary['account_category'] = category\n",
    "        #the interaction are regarding the original tweet, if the subject author is not the author of the tweet\n",
    "        #for example a retweet, or quote tweet\n",
    "        result_dictionary['num_interactions_per_tweet'] = float(interactions)\n",
    "    #consider making a second query for information only prior to the election\n",
    "    results.append(result_dictionary)\n",
    "    #same query as above, but filtering by only tweets prior to Nov 8, 2016 (the 2016 election date)\n",
    "    result1 = session.query(*sel).filter(Tweets.author == account).filter(Tweets.published_date < 1478563200).group_by(Tweets.author, Tweets.account_category).all()\n",
    "    session.close()\n",
    "    result_dictionary_b4_election = {}\n",
    "    for author, max_tweet_date, min_tweet_date, num_tweets, category, interactions in result1:\n",
    "        result_dictionary_b4_election['twitter_handle_b4'] = author\n",
    "        max_tweet_date1 = dt.datetime.fromtimestamp(max_tweet_date).strftime('%Y-%m-%d')\n",
    "        result_dictionary_b4_election['latest_tweet_b4'] = max_tweet_date1\n",
    "        min_tweet_date1 = dt.datetime.fromtimestamp(min_tweet_date).strftime('%Y-%m-%d')\n",
    "        result_dictionary_b4_election['earliest_tweet_b4'] = min_tweet_date1\n",
    "        result_dictionary_b4_election['number_of_tweets_b4'] = float(num_tweets)\n",
    "        result_dictionary_b4_election['account_category_b4'] = category\n",
    "        #the interaction are regarding the original tweet, if the subject author is not the author of the tweet\n",
    "        #for example a retweet, or quote tweet\n",
    "        result_dictionary_b4_election['num_interactions_per_tweet4_b'] = float(interactions)\n",
    "    results.append(result_dictionary_b4_election)\n",
    "    return jsonify(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Restarting with stat\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
